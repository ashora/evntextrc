{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, copy, itertools\n",
    "import numpy as np\n",
    "from bionlp.util import fs\n",
    "sys.path.append(os.path.abspath(\"C:\\\\Users\\\\yans2\\\\source\\\\py\\\\evntextrc\\\\bin\"))\n",
    "import bnlpst\n",
    "\n",
    "YEAR = '2013'\n",
    "TASK = 'bb2'\n",
    "bnlpst.DATA_PATH = 'C:\\\\Users\\\\yans2\\\\data\\\\bioevent\\\\bnlpst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spdr = bnlpst\n",
    "train_docids, dev_docids, test_docids = spdr.get_docid(dataset='train', source=YEAR, task=TASK), spdr.get_docid(dataset='dev', source=YEAR, task=TASK), spdr.get_docid(dataset='test', source=YEAR, task=TASK)\n",
    "train_raw_data = {\n",
    "    'docids':train_docids,\n",
    "    'corpus':spdr.get_corpus(train_docids, dataset='train', source=YEAR, task=TASK),\n",
    "    'preprcs':spdr.get_preprcs(train_docids, dataset='train', source=YEAR, task=TASK, method='spacy'),\n",
    "    'evnts':spdr.get_evnts(train_docids, dataset='train', source=YEAR, task=TASK)\n",
    "}\n",
    "dev_raw_data = {\n",
    "    'docids':dev_docids,\n",
    "    'corpus':spdr.get_corpus(dev_docids, dataset='dev', source=YEAR, task=TASK),\n",
    "    'preprcs':spdr.get_preprcs(dev_docids, dataset='dev', source=YEAR, task=TASK, method='spacy'),\n",
    "    'evnts':spdr.get_evnts(dev_docids, dataset='dev', source=YEAR, task=TASK)\n",
    "}\n",
    "test_raw_data = {\n",
    "    'docids':test_docids,\n",
    "    'corpus':spdr.get_corpus(test_docids, dataset='test', source=YEAR, task=TASK),\n",
    "    'preprcs':spdr.get_preprcs(test_docids, dataset='test', source=YEAR, task=TASK, method='spacy'),\n",
    "    'evnts':[[]] * len(test_docids)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name_prefix = 'bnlpst-%s-%s' % (YEAR, TASK)\n",
    "for ds_name, raw_data in zip(['%s-train' % ds_name_prefix, '%s-dev' % ds_name_prefix, '%s-test' % ds_name_prefix], [train_raw_data, dev_raw_data, test_raw_data]):\n",
    "    total_new_records = []\n",
    "    for docid, preprc, txt, evnts in zip(raw_data['docids'], raw_data['preprcs'], raw_data['corpus'], raw_data['evnts']):\n",
    "        new_records = []\n",
    "        txt = txt.strip('\\n').replace('\\n', ' ')\n",
    "        sent_bndry, words, annots, gddfs, coref = preprc\n",
    "        if (len(evnts) == 0): # Testing set\n",
    "            for locs, tp in zip(annots['loc'], annots['type']):\n",
    "                for loc in locs:\n",
    "                    newtxt.extend([txt[offset:loc[0]], '@%s$' % tp.upper()])\n",
    "                    offset = loc[1]\n",
    "            newtxt.append(txt[offset:])\n",
    "            new_records.append('%s\\t%s' % (docid, ''.join(newtxt)))\n",
    "        else: # Training and dev set\n",
    "            for eid, oprnds, evnt in zip(evnts['id'], evnts['oprnds'], evnts['type']):\n",
    "                oprids = sorted([annots['id'].index(oprnd) for oprnd in oprnds])\n",
    "                offset, newtxt = 0, []\n",
    "                for locs, tp in zip([annots['loc'][x] for x in oprids], [annots['type'][x] for x in oprids]):\n",
    "                    loc = locs[0] if (len(locs) == 1) else (np.min(locs), np.max(locs))\n",
    "                    newtxt.extend([txt[offset:loc[0]], '@%s$' % tp.upper()])\n",
    "                    offset = loc[1]\n",
    "                newtxt.append(txt[offset:])\n",
    "                new_records.append('%s-%s\\t%s\\t%s' % (docid, eid, ''.join(newtxt), evnt))\n",
    "        total_new_records.extend(new_records)\n",
    "    fs.write_file('\\n'.join(total_new_records), ds_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_name_prefix = 'bnlpst-%s-%s' % (YEAR, TASK)\n",
    "for ds_name, raw_data in zip(['%s-train.tsv' % ds_name_prefix, '%s-dev.tsv' % ds_name_prefix, '%s-test.tsv' % ds_name_prefix], [train_raw_data, dev_raw_data, test_raw_data]):\n",
    "    total_new_records = []\n",
    "    for docid, preprc, txt, evnts in zip(raw_data['docids'], raw_data['preprcs'], raw_data['corpus'], raw_data['evnts']):\n",
    "        new_records = []\n",
    "        txt = txt.rstrip('\\n').replace('\\n', ' ')\n",
    "        sent_bndry, words, annots, gddfs, coref = preprc\n",
    "        comb_pairs = ['-'.join(map(str, sorted(x))) for x in itertools.combinations(range(len(annots['id'])), 2)]\n",
    "        if (len(evnts) == 0): # Testing set\n",
    "            pass\n",
    "#             for locs, tp in zip(annots['loc'], annots['type']):\n",
    "#                 for loc in locs:\n",
    "#                     newtxt.extend([txt[offset:loc[0]], '@%s$' % tp.upper()])\n",
    "#                     offset = loc[1]\n",
    "#             newtxt.append(txt[offset:])\n",
    "#             new_records.append('%s\\t%s' % (docid, ''.join(newtxt)))\n",
    "        else: # Training and dev set\n",
    "            for eid, oprnds, evnt in zip(evnts['id'], evnts['oprnds'], evnts['type']):\n",
    "                oprids = sorted([annots['id'].index(oprnd) for oprnd in oprnds])\n",
    "                offset, newtxt = 0, []\n",
    "                for locs, tp in zip([annots['loc'][x] for x in oprids], [annots['type'][x] for x in oprids]):\n",
    "                    loc = locs[0] if (len(locs) == 1) else (np.min(locs), np.max(locs))\n",
    "                    newtxt.extend([txt[offset:max(loc[0], offset)], '@%s$' % tp.upper()])\n",
    "                    offset = max(loc[1], offset)\n",
    "                newtxt.append(txt[offset:])\n",
    "                new_records.append('%s-%s\\t%s\\t%s' % (docid, eid, ''.join(newtxt), evnt))\n",
    "                try:\n",
    "                    comb_pairs.remove('-'.join(map(str, oprids)))\n",
    "                except ValueError as e:\n",
    "                    pass\n",
    "        # Unannotated or Negative samples\n",
    "        eid_offset = 1 if (len(evnts) == 0 or len(evnts['id']) == 0) else int(evnts['id'][-1].strip('ER')) + 1\n",
    "        for eid, oprids_str in zip(['E%i'%x for x in range(eid_offset, eid_offset + len(comb_pairs))], comb_pairs):\n",
    "            oprids = map(int, oprids_str.split('-'))\n",
    "            offset, newtxt = 0, []\n",
    "            for locs, tp in zip([annots['loc'][x] for x in oprids], [annots['type'][x] for x in oprids]):\n",
    "                loc = locs[0] if (len(locs) == 1) else (np.min(locs), np.max(locs))\n",
    "                newtxt.extend([txt[offset:loc[0]], '@%s$' % tp.upper()])\n",
    "                offset = loc[1]\n",
    "            newtxt.append(txt[offset:])\n",
    "            new_records.append('%s\\t%s' % (docid, ''.join(newtxt)) if (len(evnts) == 0) else '%s-%s\\t%s\\t%s' % (docid, eid, ''.join(newtxt), 'False'))\n",
    "        total_new_records.extend(new_records)\n",
    "    fs.write_file('\\n'.join(total_new_records), ds_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
